"""
è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ç®¡ç†ç±»
è´Ÿè´£å¤„ç†è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®çš„è·å–ã€ä¿å­˜å’Œç®¡ç†
å‚è€ƒTushare daily_basicæ¥å£ï¼šhttps://tushare.pro/document/2?doc_id=32
"""
import os
import pandas as pd
import tushare
from typing import Dict, List, Optional
from ..models.daily_basic import DailyBasic
from ..mysql_manager import MySQLManager
from ..company_manager import CompanyManager


class DailyBasicManager:
    """è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ç®¡ç†ç±»"""
    
    def __init__(self, config, pro):
        """
        åˆå§‹åŒ–åŸºæœ¬é¢æ•°æ®ç®¡ç†å™¨
        
        :param config: é…ç½®å¯¹è±¡
        :param pro: Tushare Pro APIå¯¹è±¡
        """
        self.config = config
        self.pro = pro
        self.data_dir = config.get_data_dir()
        self.mysql_manager = MySQLManager(config)
        self.company_manager = CompanyManager(config)
    
    def fetch_daily_basic_data(self, ts_code: str = None, trade_date: str = None, start_date: str = None, 
                              end_date: str = None, fields: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
        
        :param ts_code: è‚¡ç¥¨ä»£ç ï¼Œæ ¼å¼å¦‚ '000001.SZ'ï¼ˆå¯é€‰ï¼Œä¸ºç©ºè¡¨ç¤ºæ‰€æœ‰è‚¡ç¥¨ï¼‰
        :param trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼ˆå¯é€‰ï¼‰
        :param start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼ˆå¯é€‰ï¼‰
        :param end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼ˆå¯é€‰ï¼‰
        :param fields: è¦è·å–çš„å­—æ®µåˆ—è¡¨ï¼Œé»˜è®¤Noneè¡¨ç¤ºæ‰€æœ‰å­—æ®µ
        :return: pandas DataFrame åŒ…å«è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼Œå¦‚æœæ— æ•°æ®è¿”å›None
        """
        try:
            # ä½¿ç”¨ DailyBasic ç±»çš„é»˜è®¤å­—æ®µ
            default_fields = DailyBasic.DEFAULT_FIELDS
            
            # åˆå¹¶ç”¨æˆ·æŒ‡å®šå­—æ®µå’Œé»˜è®¤å­—æ®µ
            if fields is None:
                fields = default_fields
            else:
                fields = list(set(fields + default_fields))
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                "fields": ",".join(fields)
            }
            
            # æ·»åŠ è‚¡ç¥¨ä»£ç å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            if ts_code:
                params["ts_code"] = ts_code
            
            # æ·»åŠ æ—¥æœŸå‚æ•°
            if trade_date:
                params["trade_date"] = trade_date
            if start_date:
                params["start_date"] = start_date
            if end_date:
                params["end_date"] = end_date
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨daily_basicæ¥å£ï¼‰
            df = self.pro.daily_basic(**params)
            
            if df.empty:
                print(f"âš ï¸  æœªæ‰¾åˆ°åŸºæœ¬é¢æ•°æ®")
                return None
            
            print(f"âœ… æˆåŠŸè·å–åŸºæœ¬é¢æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_daily_basic_data(df)
            
            return df
            
        except Exception as e:
            print(f"âŒ è·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _preprocess_daily_basic_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢„å¤„ç†åŸºæœ¬é¢æ•°æ®
        
        :param df: åŸå§‹åŸºæœ¬é¢æ•°æ®DataFrame
        :return: å¤„ç†åçš„DataFrame
        """
        # è½¬æ¢æ—¥æœŸå­—æ®µä¸ºdatetime.dateç±»å‹
        if 'trade_date' in df.columns:
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d').dt.date
        
        # è½¬æ¢å…¶ä»–æ•°å€¼å­—æ®µä¸ºfloatç±»å‹
        numeric_fields = [
            'close', 'turnover_rate', 'turnover_rate_f', 'volume_ratio',
            'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm',
            'total_share', 'float_share', 'free_share', 'total_mv', 'circ_mv'
        ]
        
        for field in numeric_fields:
            if field in df.columns:
                # é¿å…é“¾å¼èµ‹å€¼è­¦å‘Šï¼Œç›´æ¥èµ‹å€¼
                df[field] = pd.to_numeric(df[field], errors='coerce')
                # å¤„ç†NaNå€¼
                df[field] = df[field].fillna(0.0)
                # å¤„ç†infå€¼
                df[field] = df[field].replace([float('inf'), float('-inf')], 0.0)
        
        return df
    
    def save_daily_basic_data_to_csv(self, df: pd.DataFrame, filename_suffix: str = "") -> None:
        """
        ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°æœ¬åœ°CSVæ–‡ä»¶
        
        :param df: åŒ…å«åŸºæœ¬é¢æ•°æ®çš„DataFrame
        :param filename_suffix: æ–‡ä»¶ååç¼€
        """
        if df is None or df.empty:
            print("âš ï¸  æ— æ•°æ®å¯ä¿å­˜")
            return
        
        # æ„å»ºæ–‡ä»¶å
        if filename_suffix:
            filename = f"{self.data_dir}/daily_basic_{filename_suffix}.csv"
        else:
            filename = f"{self.data_dir}/daily_basic.csv"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # ä¿å­˜åˆ°CSV
        df.to_csv(filename, mode='a', index=False, encoding="utf-8-sig")
        
        print(f"âœ… åŸºæœ¬é¢æ•°æ®å·²ä¿å­˜åˆ° {filename}")

    def _get_daily_basic_table_queries(self, table_name: str, include_status_fields: bool = False) -> tuple:
        """
        å…¬å…±æ–¹æ³•ï¼šè·å–åŸºæœ¬é¢æ•°æ®è¡¨çš„åˆ›å»ºå’Œæ’å…¥æŸ¥è¯¢è¯­å¥
        
        :param table_name: æ•°æ®åº“è¡¨å
        :param include_status_fields: æ˜¯å¦åŒ…å«çŠ¶æ€å­—æ®µï¼ˆdata_status, status_reasonï¼‰
        :return: (create_table_query, insert_query, expected_columns, fill_missing_defaults)
        """
        # æ„å»ºè¡¨åˆ›å»ºè¯­å¥
        status_fields_sql = ""
        if include_status_fields:
            status_fields_sql = """
            data_status VARCHAR(10),
            status_reason VARCHAR(100),
            """
        
        create_table_query = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INT AUTO_INCREMENT PRIMARY KEY,
            ts_code VARCHAR(20),
            trade_date DATETIME,
            close FLOAT,
            turnover_rate FLOAT,
            turnover_rate_f FLOAT,
            volume_ratio FLOAT,
            pe FLOAT,
            pe_ttm FLOAT,
            pb FLOAT,
            ps FLOAT,
            ps_ttm FLOAT,
            dv_ratio FLOAT,
            dv_ttm FLOAT,
            total_share FLOAT,
            float_share FLOAT,
            free_share FLOAT,
            total_mv FLOAT,
            circ_mv FLOAT,
            {status_fields_sql}
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            UNIQUE KEY unique_stock_date (ts_code, trade_date)
        )
        """
        
        # æ„å»ºæ’å…¥è¯­å¥
        status_fields_insert = ""
        status_fields_values = ""
        status_fields_update = ""
        
        if include_status_fields:
            status_fields_insert = "data_status, status_reason,"
            status_fields_values = "%s, %s,"
            status_fields_update = """
            data_status = VALUES(data_status),
            status_reason = VALUES(status_reason),
            """
        
        insert_query = f"""
        INSERT INTO {table_name} (ts_code, trade_date, close, turnover_rate, turnover_rate_f, 
                                volume_ratio, pe, pe_ttm, pb, ps, ps_ttm, dv_ratio, dv_ttm,
                                total_share, float_share, free_share, total_mv, circ_mv, 
                                {status_fields_insert} created_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, 
                {status_fields_values} CURRENT_TIMESTAMP)
        ON DUPLICATE KEY UPDATE
            close = VALUES(close),
            turnover_rate = VALUES(turnover_rate),
            turnover_rate_f = VALUES(turnover_rate_f),
            volume_ratio = VALUES(volume_ratio),
            pe = VALUES(pe),
            pe_ttm = VALUES(pe_ttm),
            pb = VALUES(pb),
            ps = VALUES(ps),
            ps_ttm = VALUES(ps_ttm),
            dv_ratio = VALUES(dv_ratio),
            dv_ttm = VALUES(dv_ttm),
            total_share = VALUES(total_share),
            float_share = VALUES(float_share),
            free_share = VALUES(free_share),
            total_mv = VALUES(total_mv),
            circ_mv = VALUES(circ_mv),
            {status_fields_update}
            updated_at = CURRENT_TIMESTAMP
        """
        
        # æ„å»ºæœŸæœ›åˆ—å’Œé»˜è®¤å€¼
        expected_columns = [
            'ts_code', 'trade_date', 'close', 'turnover_rate', 'turnover_rate_f',
            'volume_ratio', 'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm', 'dv_ratio',
            'dv_ttm', 'total_share', 'float_share', 'free_share', 'total_mv', 'circ_mv'
        ]
        
        fill_missing_defaults = {
            'close': 0.0, 'turnover_rate': 0.0, 'turnover_rate_f': 0.0, 'volume_ratio': 0.0,
            'pe': 0.0, 'pe_ttm': 0.0, 'pb': 0.0, 'ps': 0.0, 'ps_ttm': 0.0, 'dv_ratio': 0.0,
            'dv_ttm': 0.0, 'total_share': 0.0, 'float_share': 0.0, 'free_share': 0.0,
            'total_mv': 0.0, 'circ_mv': 0.0
        }
        
        if include_status_fields:
            expected_columns.extend(['data_status', 'status_reason'])
            fill_missing_defaults.update({
                'data_status': 'success',
                'status_reason': ''
            })
        
        return create_table_query, insert_query, expected_columns, fill_missing_defaults

    def _save_daily_basic_data_to_mysql(self, df: pd.DataFrame, table_name: str = "daily_basic", 
                                       batch_size: int = 50) -> bool:
        """
        ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°MySQLæ•°æ®åº“ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        
        :param df: åŒ…å«åŸºæœ¬é¢æ•°æ®çš„DataFrame
        :param table_name: æ•°æ®åº“è¡¨å
        :param batch_size: æ‰¹é‡å¤„ç†å¤§å°
        :return: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        if df is None or df.empty:
            print("âš ï¸  æ— æ•°æ®å¯ä¿å­˜åˆ°MySQL")
            return False
        
        try:
            # è·å–è¡¨åˆ›å»ºå’Œæ’å…¥æŸ¥è¯¢
            create_table_query, insert_query, expected_columns, fill_missing_defaults = \
                self._get_daily_basic_table_queries(table_name)
            
            # åˆ›å»ºè¡¨
            self.mysql_manager.execute_query(create_table_query)
            
            # å‡†å¤‡æ•°æ®
            data_to_insert = []
            for _, row in df.iterrows():
                row_data = {}
                for col in expected_columns:
                    if col in row:
                        row_data[col] = row[col]
                    else:
                        row_data[col] = fill_missing_defaults.get(col, None)
                
                # è½¬æ¢trade_dateä¸ºå­—ç¬¦ä¸²æ ¼å¼
                if 'trade_date' in row_data and hasattr(row_data['trade_date'], 'strftime'):
                    row_data['trade_date'] = row_data['trade_date'].strftime('%Y-%m-%d')
                
                data_to_insert.append(tuple(row_data[col] for col in expected_columns))
            
            # æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆæ¯batch_sizeæ¡ä¿å­˜ä¸€æ¬¡ï¼‰
            total_records = len(data_to_insert)
            success_count = 0
            
            for i in range(0, total_records, batch_size):
                batch = data_to_insert[i:i + batch_size]
                try:
                    self.mysql_manager.execute_many(insert_query, batch)
                    success_count += len(batch)
                    # print(f"âœ… å·²æ‰¹é‡ä¿å­˜ {len(batch)} æ¡åŸºæœ¬é¢æ•°æ®åˆ°MySQL ({i + len(batch)}/{total_records})")
                except Exception as e:
                    print(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
            
            print(f"âœ… æˆåŠŸä¿å­˜ {success_count}/{total_records} æ¡åŸºæœ¬é¢æ•°æ®åˆ°MySQLè¡¨ {table_name}")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°MySQLå¤±è´¥: {e}")
            return False

    def fetch_daily_basic_data_by_trade_date(self, trade_date: str, fields: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡å®šäº¤æ˜“æ—¥çš„æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
        
        :param trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        :param fields: è¦è·å–çš„å­—æ®µåˆ—è¡¨ï¼Œé»˜è®¤Noneè¡¨ç¤ºæ‰€æœ‰å­—æ®µ
        :return: pandas DataFrame åŒ…å«æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼Œå¦‚æœæ— æ•°æ®è¿”å›None
        """
        try:
            # ä½¿ç”¨ DailyBasic ç±»çš„é»˜è®¤å­—æ®µ
            default_fields = DailyBasic.DEFAULT_FIELDS
            
            # åˆå¹¶ç”¨æˆ·æŒ‡å®šå­—æ®µå’Œé»˜è®¤å­—æ®µ
            if fields is None:
                fields = default_fields
            else:
                fields = list(set(fields + default_fields))
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                "trade_date": trade_date,
                "fields": ",".join(fields)
            }
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨daily_basicæ¥å£ï¼‰
            df = self.pro.daily_basic(**params)
            
            if df.empty:
                print(f"âš ï¸  æœªæ‰¾åˆ°äº¤æ˜“æ—¥ {trade_date} çš„åŸºæœ¬é¢æ•°æ®")
                return None
            
            print(f"âœ… æˆåŠŸè·å–äº¤æ˜“æ—¥ {trade_date} çš„åŸºæœ¬é¢æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_daily_basic_data(df)
            
            return df
            
        except Exception as e:
            print(f"âŒ è·å–äº¤æ˜“åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            return None

    def fetch_all_stocks_daily_basic_period(self, start_date: str, end_date: str, 
                                           save_to_mysql: bool = True, table_name: str = "daily_basic",
                                           batch_size: int = 50) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„åŸºæœ¬é¢æ•°æ®ï¼ˆæŒ‰ç…§Tushareæ¨èçš„æ–¹å¼ï¼šæŒ‰æ—¥æœŸå¾ªç¯ï¼‰
        
        :param start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        :param end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        :param save_to_mysql: æ˜¯å¦ä¿å­˜åˆ°MySQLæ•°æ®åº“
        :param table_name: MySQLè¡¨å
        :param batch_size: æ‰¹é‡ä¿å­˜å¤§å°
        :return: åŒ…å«æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®çš„DataFrame
        """
        print(f"ğŸš€ å¼€å§‹è·å–æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ® ({start_date} åˆ° {end_date})")
        
        # è·å–äº¤æ˜“æ—¥åˆ—è¡¨
        trade_dates = self._get_trade_dates(start_date, end_date)
        
        if not trade_dates:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥")
            return pd.DataFrame()
        
        all_data = []
        total_dates = len(trade_dates)
        
        # æŒ‰äº¤æ˜“æ—¥å¾ªç¯è·å–æ•°æ®
        for i, trade_date in enumerate(trade_dates, 1):
            print(f"ğŸ“… æ­£åœ¨å¤„ç†äº¤æ˜“æ—¥ {i}/{total_dates}: {trade_date}")
            
            try:
                # è·å–è¯¥äº¤æ˜“æ—¥çš„æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
                df = self.fetch_daily_basic_data_by_trade_date(trade_date)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    print(f"âœ… æˆåŠŸè·å–äº¤æ˜“æ—¥ {trade_date} çš„åŸºæœ¬é¢æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
                    
                    # å¦‚æœå¯ç”¨äº†MySQLä¿å­˜ï¼Œç«‹å³ä¿å­˜è¿™æ‰¹æ•°æ®
                    if save_to_mysql:
                        self._save_daily_basic_data_to_mysql(df, table_name, batch_size)
                    
                    # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆæŒ‰æ—¥æœŸï¼‰
                    # self.save_daily_basic_data_to_csv(df, trade_date)
                else:
                    print(f"âš ï¸  äº¤æ˜“æ—¥ {trade_date} æ— åŸºæœ¬é¢æ•°æ®")
                    
            except Exception as e:
                print(f"âŒ å¤„ç†äº¤æ˜“æ—¥ {trade_date} æ—¶å‡ºé”™: {e}")
                continue
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ… æˆåŠŸè·å–æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼Œå…± {len(combined_df)} æ¡è®°å½•")
            
            # ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°CSV
            csv_filename = f"daily_basic_all_stocks_{start_date}_{end_date}"
            # self.save_daily_basic_data_to_csv(combined_df, csv_filename)
            
            return combined_df
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•åŸºæœ¬é¢æ•°æ®")
            return pd.DataFrame()

    def _get_trade_dates(self, start_date: str, end_date: str) -> List[str]:
        """
        è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„äº¤æ˜“æ—¥åˆ—è¡¨
        
        :param start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        :param end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        :return: äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆæ ¼å¼YYYYMMDDï¼‰
        """
        try:
            # è·å–äº¤æ˜“æ—¥å†
            df = self.pro.trade_cal(exchange='', start_date=start_date, end_date=end_date, fields='cal_date,is_open')
            
            if df.empty:
                print(f"âš ï¸  æœªæ‰¾åˆ°äº¤æ˜“æ—¥å†æ•°æ® ({start_date} åˆ° {end_date})")
                return []
            
            # ç­›é€‰å¼€å¸‚æ—¥
            trade_dates = df[df['is_open'] == 1]['cal_date'].tolist()
            
            if not trade_dates:
                print(f"âš ï¸  åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰å¼€å¸‚æ—¥")
                return []
            
            print(f"âœ… æ‰¾åˆ° {len(trade_dates)} ä¸ªäº¤æ˜“æ—¥")
            return trade_dates
            
        except Exception as e:
            print(f"âŒ è·å–äº¤æ˜“æ—¥å¤±è´¥: {e}")
            return []


# # if __name__ == "__main__":
#     # ç¤ºä¾‹ç”¨æ³•
#     import tushare as ts
#     from parse_config import ParseConfig
    
#     # åˆå§‹åŒ–é…ç½®å’ŒTushare Pro API
#     config = ParseConfig()
#     pro = ts.pro_api(config.get_token())
    
#     # åˆ›å»ºåŸºæœ¬é¢æ•°æ®ç®¡ç†å™¨
#     daily_basic_manager = DailyBasicManager(config, pro)
    
#     # # ç¤ºä¾‹1ï¼šè·å–å•åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®
#     # print("=== ç¤ºä¾‹1ï¼šè·å–å•åªè‚¡ç¥¨åŸºæœ¬é¢æ•°æ® ===")
#     # df_single = daily_basic_manager.fetch_daily_basic_data("000001.SZ", start_date="20250927", end_date="20251001")
#     # if df_single is not None:
#     #     daily_basic_manager.save_daily_basic_data_to_csv(df_single, "000001.SZ")
    
#     # ç¤ºä¾‹2ï¼šè·å–æ‰€æœ‰è‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®ï¼ˆæŒ‰æ—¥æœŸå¾ªç¯ï¼‰
#     print("\n=== ç¤ºä¾‹2ï¼šè·å–æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬é¢æ•°æ® ===")
#     df_all = daily_basic_manager.fetch_all_stocks_daily_basic_period(
#         start_date="20250101", 
#         end_date="20251001",
#         save_to_mysql=True,
#         batch_size=50
#     )
    
#     print("âœ… åŸºæœ¬é¢æ•°æ®è·å–å®Œæˆï¼")